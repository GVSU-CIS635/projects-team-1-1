{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GVSU-CIS635/projects-team-1-1/blob/main/Project_updated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing packages"
      ],
      "metadata": {
        "id": "Z1k6xILOGZ2s"
      },
      "id": "Z1k6xILOGZ2s"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "608743e5",
      "metadata": {
        "id": "608743e5",
        "outputId": "739c6bee-f174-48d6-c27e-8b73cab27556",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optuna in /usr/local/lib/python3.12/dist-packages (4.6.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.17.2)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.12/dist-packages (from optuna) (6.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.44)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.3)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "# Installing Optuna first as it is an external dependency\n",
        "!pip install optuna\n",
        "\n",
        "# --- CONSOLIDATED IMPORTS ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import optuna\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# SKLEARN MODULES\n",
        "from sklearn.base import clone\n",
        "from sklearn.model_selection import (\n",
        "    train_test_split,\n",
        "    GridSearchCV,\n",
        "    StratifiedKFold,\n",
        "    StratifiedShuffleSplit,\n",
        "    cross_validate\n",
        ")\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import (\n",
        "    StandardScaler,\n",
        "    OneHotEncoder,\n",
        "    label_binarize # Although not used in final code, kept for completeness\n",
        ")\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# SKLEARN MODELS\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# SKLEARN METRICS\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    roc_auc_score,\n",
        "    classification_report,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    confusion_matrix,\n",
        "    precision_recall_curve, # Needed for threshold tuning\n",
        "    roc_curve,\n",
        "    auc\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Collection"
      ],
      "metadata": {
        "id": "QaOYRVCebyBd"
      },
      "id": "QaOYRVCebyBd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d5640aa",
      "metadata": {
        "id": "9d5640aa"
      },
      "outputs": [],
      "source": [
        "# Data Loading\n",
        "df_train = pd.read_csv(\"https://raw.githubusercontent.com/GVSU-CIS635/projects-team-1-1/main/data/train.csv\", sep=\";\", skipinitialspace=True)\n",
        "df_test = pd.read_csv(\"https://raw.githubusercontent.com/GVSU-CIS635/projects-team-1-1/main/data/test.csv\", sep=\";\", skipinitialspace=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Missing Values"
      ],
      "metadata": {
        "id": "g8W4jDBegCU9"
      },
      "id": "g8W4jDBegCU9"
    },
    {
      "cell_type": "code",
      "source": [
        "# looking for missing data\n",
        "print(\"Missing values found in Train\\n\", df_train.isnull().sum())\n",
        "\n",
        "# looking for duplicate data\n",
        "print(\"Duplicates found in Train \", df_train.duplicated().sum())\n",
        "\n",
        "print(\"---------------------------------------------------------\")\n",
        "# looking for missing data\n",
        "print(\"Missing values found in test\\n\", df_test.isnull().sum())\n",
        "\n",
        "# looking for duplicate data\n",
        "print(\"Duplicates found in test:\", df_test.duplicated().sum())"
      ],
      "metadata": {
        "id": "nRcjCCjUgDgd"
      },
      "id": "nRcjCCjUgDgd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even though the source has already stated that the data has no missing values or duplicates, it is still good to verify this, since these steps affect all the later processes."
      ],
      "metadata": {
        "id": "wt_gJR3IgO3j"
      },
      "id": "wt_gJR3IgO3j"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checking if Test is a subset of Train"
      ],
      "metadata": {
        "id": "Mmnm8TLHXjbf"
      },
      "id": "Mmnm8TLHXjbf"
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a boolean mask: for each test row, check if it appears in train\n",
        "mask = df_test.merge(df_train.drop_duplicates(), how='left', indicator=True)['_merge'] == 'both'"
      ],
      "metadata": {
        "id": "RzCNJ8FEXacO"
      },
      "id": "RzCNJ8FEXacO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_test = len(df_test)\n",
        "num_matches = mask.sum()\n",
        "num_missing = num_test - num_matches\n",
        "percent_match = num_matches / num_test * 100\n",
        "\n",
        "print(f\"Test rows: {num_test}\")\n",
        "print(f\"Rows that appear in train: {num_matches}\")\n",
        "print(f\"Rows NOT found in train: {num_missing}\")\n",
        "print(f\"Percent of test that is in train: {percent_match:.2f}%\")"
      ],
      "metadata": {
        "id": "OlmF0k4hXvz8"
      },
      "id": "OlmF0k4hXvz8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since test is a subset of train we can use training data to split into train(70%) and test(30%) and use it for models."
      ],
      "metadata": {
        "id": "0ONeWO_nX6aE"
      },
      "id": "0ONeWO_nX6aE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Group Classification"
      ],
      "metadata": {
        "id": "4Cq01kkbW3dt"
      },
      "id": "4Cq01kkbW3dt"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define feature groups\n",
        "num_features = [\"age\", \"balance\", \"day\", \"campaign\", \"pdays_numeric\", \"previous\"]\n",
        "cat_features = [\"job\", \"marital\", \"education\", \"contact\", \"month\", \"poutcome\"]\n",
        "bin_features = [\"default\", \"housing\", \"loan\"]"
      ],
      "metadata": {
        "id": "qTEV5MeWVJrr"
      },
      "id": "qTEV5MeWVJrr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Cleaning"
      ],
      "metadata": {
        "id": "YJm4t12MYNGr"
      },
      "id": "YJm4t12MYNGr"
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_external_minimal(df_ext: pd.DataFrame) -> pd.DataFrame:\n",
        "\n",
        "    # making a copy\n",
        "    dfx = df_ext.copy()\n",
        "\n",
        "    # Normalize headers\n",
        "    dfx.columns = dfx.columns.str.strip().str.lower()\n",
        "\n",
        "    if \"duration\" in dfx.columns:\n",
        "        dfx = dfx.drop(columns=[\"duration\"])\n",
        "\n",
        "    # Fix mixed-type categorical columns\n",
        "    cat_cols_train = df_train.select_dtypes(include=[\"object\"]).columns.tolist()\n",
        "    for c in cat_cols_train:\n",
        "        df_train[c] = df_train[c].astype(str)\n",
        "\n",
        "    dfx[\"contacted_before\"] = (dfx[\"pdays\"] != -1).astype(int)\n",
        "    dfx[\"pdays_numeric\"]    = dfx[\"pdays\"].replace(-1, 0)\n",
        "    for col in bin_features + [\"y\"]:\n",
        "        if col in dfx.columns:\n",
        "            dfx[col] = dfx[col].map({\"yes\": 1, \"no\": 0})\n",
        "    return dfx"
      ],
      "metadata": {
        "id": "v5OAEXuWfv0k"
      },
      "id": "v5OAEXuWfv0k",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = prepare_external_minimal(df_train)\n",
        "df_test  = prepare_external_minimal(df_test)"
      ],
      "metadata": {
        "id": "NmDnhO0sgf-G"
      },
      "id": "NmDnhO0sgf-G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HL\n",
        "gnb_ex_df_test = df_test.copy()"
      ],
      "metadata": {
        "id": "Syt-80KI9_SH"
      },
      "id": "Syt-80KI9_SH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e2c29ce0",
      "metadata": {
        "id": "e2c29ce0"
      },
      "source": [
        "# Numeric Features Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e433947",
      "metadata": {
        "id": "3e433947"
      },
      "outputs": [],
      "source": [
        "# Box plot\n",
        "plt.figure(figsize=(12,8))\n",
        "for i, col in enumerate(num_features, 1):\n",
        "    plt.subplot(3, 3, i)\n",
        "    sns.boxplot(x = df_train[col], color=\"skyblue\")\n",
        "    plt.title(f\"{col}\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3041529",
      "metadata": {
        "id": "f3041529"
      },
      "outputs": [],
      "source": [
        "# Distribution\n",
        "plt.figure(figsize=(12,8))\n",
        "for i, col in enumerate(num_features, 1):\n",
        "    plt.subplot(3, 3, i)\n",
        "    sns.histplot(df_train[col], bins=30, kde=True)\n",
        "    plt.title(f\"{col}\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f61abbb",
      "metadata": {
        "id": "9f61abbb"
      },
      "outputs": [],
      "source": [
        "# Correlation\n",
        "corr = df_train[num_features].corr()\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title(\"Correlation Heatmap (Numeric Features)\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "946909b3",
      "metadata": {
        "id": "946909b3"
      },
      "source": [
        "We are trying to understand the numerical features in this section:\n",
        "- How are the features distributed? Do they show any skewness?\n",
        "- What are the correlations among the features?\n",
        "- This will help us make better judgments on whether remove outliers, normalize, or remove similar features.\n",
        "\n",
        "From the box plot and the distribution graphs\n",
        "- Age       : that looks ok, with outliers\n",
        "- Balance   : most people fill in 0, maybe people over look this section\n",
        "- Day       : no outliers, good distribution\n",
        "- Duration  : dropped due to data leakage\n",
        "- Campaign  : more than 6 calls during a campain and that would be too many call for one personm > skewed right\n",
        "- Pdays = contacted_before + pdays_numeric bc -1 means client was not previously contacted, there are too many -1 (81.7%) > skewed right\n",
        "- Previous  : most clients are new (81.7%) > skewed right\n",
        "\n",
        "Notes:\n",
        "- From the correlation heatmap, most features are not correlated to anothers so we can keep them all\n",
        "\n",
        "Actions\n",
        "- Age           : StandardScaler\n",
        "- Balance       : Skewed right with negative values > StandardScaler\n",
        "- Day           : StandardScaler\n",
        "- Campaign      : log1p transformed > StandardScaler\n",
        "- pdays_numeric : log1p transformed > StandardScaler\n",
        "- previous      : log1p transformed > StandardScaler\n",
        "\n",
        "What is log1p transform?\n",
        "- Reduces skewness\n",
        "- Makes the distribution more normal-like\n",
        "- Helps with logistic regression"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11963bac",
      "metadata": {
        "id": "11963bac"
      },
      "source": [
        "# Categorical features Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64918ab6",
      "metadata": {
        "id": "64918ab6"
      },
      "outputs": [],
      "source": [
        "# Graphs\n",
        "# Number of plots\n",
        "n     = len(cat_features)         # 10\n",
        "ncols = 3                         # 3 columns\n",
        "nrows = n // ncols + 1            # 10 // 3 + 1 = 4\n",
        "\n",
        "plt.figure(figsize=(5*ncols, 4*nrows))\n",
        "\n",
        "for i, col in enumerate(cat_features, 1):\n",
        "    plt.subplot(nrows, ncols, i)\n",
        "    ax = sns.countplot(\n",
        "        x=col,\n",
        "        hue=col,\n",
        "        data=df_train,\n",
        "        palette='pastel',\n",
        "        order=df_train[col].value_counts().index,\n",
        "        legend=False\n",
        "    )\n",
        "\n",
        "    total = len(df_train)\n",
        "\n",
        "    # find tallest bar to give extra y-axis space\n",
        "    max_height = max(p.get_height() for p in ax.patches)\n",
        "    ax.set_ylim(0, max_height * 1.10)  # 15% space above bars\n",
        "\n",
        "    # annotate each bar\n",
        "    for p in ax.patches:\n",
        "        count = p.get_height()\n",
        "        percentage = 100 * count / total\n",
        "\n",
        "        # annotate text slightly above bar top\n",
        "        ax.annotate(\n",
        "            f'{percentage:.1f}%',\n",
        "            xy=(p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "            xytext=(0, 6),                  # 6 points above the bar\n",
        "            textcoords='offset points',\n",
        "            ha='center', va='bottom',\n",
        "            fontsize=9, color='black'\n",
        "        )\n",
        "\n",
        "    plt.title(col.capitalize(), fontsize=11)\n",
        "    plt.xlabel('')\n",
        "    plt.ylabel('')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5aab569b",
      "metadata": {
        "id": "5aab569b"
      },
      "source": [
        "From the bar charts:\n",
        "- There are some dominant responses among the features; during training, the classes(freatures) will therefore be weighted.\n",
        "\n",
        "What is Cramér’s V correlation coefficient?\n",
        "- It measures the strength of association between two categorical variables.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "feb767b8",
      "metadata": {
        "id": "feb767b8"
      },
      "outputs": [],
      "source": [
        "def cramers_v(x, y):\n",
        "    table = pd.crosstab(x, y)\n",
        "    chi2  = chi2_contingency(table, correction = False)[0]\n",
        "    n     = table.sum().sum()\n",
        "    k     = min(table.shape)\n",
        "    return np.sqrt(chi2 / (n * (k - 1)))\n",
        "\n",
        "# build the matrix\n",
        "cramers = pd.DataFrame(index = cat_features, columns=cat_features, dtype=float)\n",
        "\n",
        "for c1 in cat_features:\n",
        "    for c2 in cat_features:\n",
        "        cramers.loc[c1, c2] = cramers_v(df_train[c1], df_train[c2])\n",
        "\n",
        "# visualize\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cramers.astype(float), annot=True, cmap='Blues', fmt=\".2f\")\n",
        "plt.title(\"Cramér's V Correlation between Categorical Features\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29e94cee",
      "metadata": {
        "id": "29e94cee"
      },
      "source": [
        "From the Cramer's V heat map\n",
        "- There is no strong correlation among features\n",
        "- 0.5 and 0.46 are the two most significant values\n",
        "- education and job are kind of related - if you are in school > you are a student\n",
        "- month and housing - in the summer, people are just happy to go buy a house\n",
        "- month and contact - in the summer, people are just happy to pickup the phone"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train/Test split"
      ],
      "metadata": {
        "id": "gmKxqtBAaHe4"
      },
      "id": "gmKxqtBAaHe4"
    },
    {
      "cell_type": "code",
      "source": [
        "# 70% train / 30% internal test (keep separate test.csv untouched)\n",
        "X = df_train[num_features + cat_features + bin_features]\n",
        "y = df_train[\"y\"].astype(int)"
      ],
      "metadata": {
        "id": "gVFVXuyoaDvz"
      },
      "id": "gVFVXuyoaDvz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
        "\n",
        "for train_idx, test_idx in sss.split(X, y):\n",
        "  X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "  y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]"
      ],
      "metadata": {
        "id": "UOEpYEhMbjMt"
      },
      "id": "UOEpYEhMbjMt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HL\n",
        "gnb_X_train = X_train.copy()\n",
        "gnb_X_test  = X_train.copy()\n",
        "gnb_y_train = y_train.copy()\n",
        "gnb_y_test  = y_train.copy()"
      ],
      "metadata": {
        "id": "6mZVegy3RjUq"
      },
      "id": "6mZVegy3RjUq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "oKbSMovLaOqh"
      },
      "id": "oKbSMovLaOqh"
    },
    {
      "cell_type": "code",
      "source": [
        "# Imputer + Scaler are placed in every pipeline to avoid leakage and for consistent processing.\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "scaler  = StandardScaler()"
      ],
      "metadata": {
        "id": "L2419sQDuHuJ"
      },
      "id": "L2419sQDuHuJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_transformer = Pipeline(\n",
        "  steps=[\n",
        "  (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "  (\"scaler\", StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(\n",
        "  steps=[\n",
        "  (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "  (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
        "])"
      ],
      "metadata": {
        "id": "xg9t3GcPuwaS"
      },
      "id": "xg9t3GcPuwaS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing transformer\n",
        "preprocessor = ColumnTransformer(\n",
        "  transformers=[\n",
        "    ('num', numeric_transformer, num_features),\n",
        "    ('cat', categorical_transformer, cat_features),\n",
        "    ('bin', 'passthrough', bin_features),\n",
        "  ],\n",
        ")\n",
        "\n",
        "# HL\n",
        "preprocessor_gnb = ColumnTransformer(\n",
        "  transformers=[\n",
        "    ('num', numeric_transformer, num_features),\n",
        "    ('cat', categorical_transformer, cat_features),\n",
        "    ('bin', 'passthrough', bin_features),\n",
        "  ],\n",
        ")"
      ],
      "metadata": {
        "id": "b8z4JT5OaRMx"
      },
      "id": "b8z4JT5OaRMx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### After the proprocessing, everyone can do what ever they want"
      ],
      "metadata": {
        "id": "elUmgcsMANwE"
      },
      "id": "elUmgcsMANwE"
    },
    {
      "cell_type": "code",
      "source": [
        "# train.csv > train_set + test_set (70/30) use shuffle\n",
        "\n",
        "# 1. internal testing for the test_set\n",
        "\n",
        "# 2. external testing using test.csv\n",
        "\n",
        "# Compare 1 vs 2"
      ],
      "metadata": {
        "id": "cVKCQMXNAZJF"
      },
      "id": "cVKCQMXNAZJF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kelvin - XGBOOST"
      ],
      "metadata": {
        "id": "6kyHVqgr_yFp"
      },
      "id": "6kyHVqgr_yFp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model & Optuna Optimization"
      ],
      "metadata": {
        "id": "lBeTBQtGIFMQ"
      },
      "id": "lBeTBQtGIFMQ"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Ensure necessary function is defined ---\n",
        "def _best_threshold_for_f1(y_true, y_prob):\n",
        "    \"\"\"Finds the optimal threshold that maximizes the standard F1 score (F1-binary).\"\"\"\n",
        "    p, r, thr = precision_recall_curve(y_true, y_prob)\n",
        "    f1 = 2 * (p * r) / (p + r + 1e-12)\n",
        "    idx = int(np.nanargmax(f1))\n",
        "    return 0.5 if idx >= len(thr) else float(thr[idx])\n",
        "\n",
        "# Class imbalance from TRAIN only (uses globally defined y_train)\n",
        "neg, pos = np.bincount(y_train)\n",
        "global_scale_pos_weight = neg / max(pos, 1)\n",
        "\n",
        "def build_pipeline(trial):\n",
        "    \"\"\"\n",
        "    Defines the XGBoost model and optimization space.\n",
        "    Uses 'logloss' for internal fitting to avoid the 'Unknown metric function f1' error.\n",
        "    \"\"\"\n",
        "    params = {\n",
        "        \"random_state\": 1972,\n",
        "        \"tree_method\": \"hist\",\n",
        "        \"eval_metric\": \"logloss\", # FIX: Use logloss for internal XGBoost fitting\n",
        "        \"scale_pos_weight\": global_scale_pos_weight,\n",
        "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 300, 1200),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 7),\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
        "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
        "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
        "        \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 1.0, 10.0),\n",
        "        \"gamma\": trial.suggest_float(\"gamma\", 0.0, 0.5),\n",
        "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.0, 1.0),\n",
        "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1.0, 10.0),\n",
        "        \"n_jobs\": -1,\n",
        "    }\n",
        "    model = XGBClassifier(**params)\n",
        "    # preprocessor is defined in the main project setup (Section 4)\n",
        "    return Pipeline(steps=[('preprocess', preprocessor), ('xgb', model)])\n",
        "\n",
        "def objective(trial):\n",
        "    \"\"\"Maximizes the mean F1 score using threshold tuning within CV.\"\"\"\n",
        "    pipe = build_pipeline(trial)\n",
        "    # Use copies for robust pandas indexing with SKF\n",
        "    X_train_pd = X_train.copy()\n",
        "    y_train_pd = y_train.copy()\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1972)\n",
        "\n",
        "    f1s, accs, aucs = [], [], []\n",
        "    for tr_idx, va_idx in skf.split(X_train_pd, y_train_pd):\n",
        "        Xtr, Xva = X_train_pd.iloc[tr_idx], X_train_pd.iloc[va_idx]\n",
        "        ytr, yva = y_train_pd.iloc[tr_idx], y_train_pd.iloc[va_idx]\n",
        "\n",
        "        pipe.fit(Xtr, ytr)\n",
        "        va_prob = pipe.predict_proba(Xva)[:, 1]\n",
        "\n",
        "        # 1. Tune threshold on this fold's validation set (F1 binary target)\n",
        "        thr = _best_threshold_for_f1(yva, va_prob)\n",
        "\n",
        "        # 2. Calculate metrics using the optimized threshold\n",
        "        y_pred = (va_prob >= thr).astype(int)\n",
        "\n",
        "        f1 = f1_score(yva, y_pred)\n",
        "        acc = accuracy_score(yva, y_pred)\n",
        "        auc = roc_auc_score(yva, va_prob)\n",
        "\n",
        "        f1s.append(f1); accs.append(acc); aucs.append(auc)\n",
        "\n",
        "    trial.set_user_attr(\"mean_accuracy\", float(np.mean(accs)))\n",
        "    trial.set_user_attr(\"mean_auc\", float(np.mean(aucs)))\n",
        "    return float(np.mean(f1s))\n",
        "\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "print(\"\\nStarting Optuna optimization to maximize F1 score (binary)...\")\n",
        "study.optimize(objective, n_trials=30, show_progress_bar=True)\n",
        "\n",
        "print(\"\\nBest trial params:\", study.best_trial.params)\n",
        "print(\"CV-best F1 (opt objective):\", study.best_value)\n",
        "print(\"CV mean Accuracy (best):\", study.best_trial.user_attrs.get(\"mean_accuracy\"))\n",
        "print(\"CV mean AUC-ROC (best):\", study.best_trial.user_attrs.get(\"mean_auc\"))\n",
        "\n",
        "# Store the best pipeline for final evaluation\n",
        "best_xgb_pipe = build_pipeline(study.best_trial)"
      ],
      "metadata": {
        "id": "UMlpkupLC_-X"
      },
      "id": "UMlpkupLC_-X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Refit best pipeline + Internal Test Evaluation"
      ],
      "metadata": {
        "id": "NMmQ15CEIQjt"
      },
      "id": "NMmQ15CEIQjt"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"XGBOOST INTERNAL TEST RESULTS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# 1. Split train data for internal threshold tuning (Validation set)\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.15, random_state=1972)\n",
        "(tr_idx, va_idx) = next(sss.split(X_train, y_train))\n",
        "Xtr, Xva = X_train.iloc[tr_idx], X_train.iloc[va_idx]\n",
        "ytr, yva = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n",
        "\n",
        "# 2. Fit the best model using the full training partition (Xtr, ytr)\n",
        "best_xgb_pipe.fit(Xtr, ytr)\n",
        "\n",
        "# 3. Tune the threshold using the validation set (Xva, yva)\n",
        "val_prob = best_xgb_pipe.predict_proba(Xva)[:, 1]\n",
        "best_thr = _best_threshold_for_f1(yva, val_prob)\n",
        "\n",
        "# 4. Evaluate on the internal 30% Test set (X_test, y_test)\n",
        "test_prob = best_xgb_pipe.predict_proba(X_test)[:, 1]\n",
        "test_pred = (test_prob >= best_thr).astype(int)\n",
        "\n",
        "print(\"\\n=== Internal 30% Test (Optuna-tuned, thr tuned on val) ===\")\n",
        "print(\"F1:        {:.4f}\".format(f1_score(y_test, test_pred)))\n",
        "print(\"Accuracy: {:.4f}\".format(accuracy_score(y_test, test_pred)))\n",
        "print(\"AUC-ROC:   {:.4f}\".format(roc_auc_score(y_test, test_prob)))\n",
        "print(\"\\nClassification report:\\n\", classification_report(y_test, test_pred, digits=4))"
      ],
      "metadata": {
        "id": "SEmUXM0HIRC2"
      },
      "id": "SEmUXM0HIRC2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### External Final Test Evaluation"
      ],
      "metadata": {
        "id": "NgJU4ieWIjMl"
      },
      "id": "NgJU4ieWIjMl"
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Select features and target from the cleaned df_test\n",
        "X_ext = df_test[num_features + cat_features + bin_features]\n",
        "y_ext = df_test['y'].astype(int)\n",
        "\n",
        "# 2. Predict on the external test set\n",
        "ext_prob = best_xgb_pipe.predict_proba(X_ext)[:, 1]\n",
        "ext_pred = (ext_prob >= best_thr).astype(int)\n",
        "\n",
        "print(\"\\n=== External Final Test (threshold from train-val) ===\")\n",
        "print(\"F1:        {:.4f}\".format(f1_score(y_ext, ext_pred)))\n",
        "print(\"Accuracy: {:.4f}\".format(accuracy_score(y_ext, ext_pred)))\n",
        "print(\"AUC-ROC:   {:.4f}\".format(roc_auc_score(y_ext, ext_prob)))\n",
        "print(\"\\nClassification report:\\n\", classification_report(y_ext, ext_pred, digits=4))"
      ],
      "metadata": {
        "id": "t6J1PBA7IkSF"
      },
      "id": "t6J1PBA7IkSF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sri - Logistic Regression\n",
        "# San - Random Forest"
      ],
      "metadata": {
        "id": "JDKjhMgBUtio"
      },
      "id": "JDKjhMgBUtio"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipeline"
      ],
      "metadata": {
        "id": "F47R_2RNdAoO"
      },
      "id": "F47R_2RNdAoO"
    },
    {
      "cell_type": "code",
      "source": [
        "# creating pipelines\n",
        "lr_pipe = Pipeline([\n",
        "  ('preprocess', preprocessor),\n",
        "  ('clf', LogisticRegression(max_iter=2000))\n",
        "])\n",
        "\n",
        "rf_pipe = Pipeline([\n",
        "  ('preprocess', preprocessor),\n",
        "  ('clf', RandomForestClassifier(random_state=42, class_weight='balanced', n_jobs=-1))\n",
        "])"
      ],
      "metadata": {
        "id": "DPKe0KFZdCF9"
      },
      "id": "DPKe0KFZdCF9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parameter Grid"
      ],
      "metadata": {
        "id": "X8tcrqzIdFeH"
      },
      "id": "X8tcrqzIdFeH"
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters for Logistic Regression\n",
        "lr_param_grid = {\n",
        "  'clf__C': [0.01, 0.1, 1, 10, 100],\n",
        "}\n",
        "\n",
        "rf_param_grid = {\n",
        "    'clf__n_estimators': [100, 150],          # Moderate number\n",
        "    'clf__max_depth': [12, 15],               # Limited depth\n",
        "    'clf__min_samples_split': [10, 15],       # Higher values = less overfitting\n",
        "    'clf__min_samples_leaf': [5, 7],          # Larger leaves = smoother model\n",
        "    'clf__max_features': ['sqrt'],\n",
        "}\n",
        "\n",
        "\n",
        "# Optional\n",
        "# print(\"\\n[Hyperparameter Grid]\")\n",
        "# for param, values in rf_param_grid.items():\n",
        "#     print(f\"  {param}: {values}\")\n",
        "# print(f\"Total combinations: {len(rf_param_grid['clf__n_estimators']) * len(rf_param_grid['clf__max_depth']) * len(rf_param_grid['clf__min_samples_split'])}\")\n"
      ],
      "metadata": {
        "id": "7JeLa0TFdIpv"
      },
      "id": "7JeLa0TFdIpv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## StratifiedKFold"
      ],
      "metadata": {
        "id": "O4jAfY2ddqM7"
      },
      "id": "O4jAfY2ddqM7"
    },
    {
      "cell_type": "code",
      "source": [
        "# creating 2 seperate because this avoids optimistic bias because you evaluate on folds that the model has not seen during hyperparameter tuning.\n",
        "# use the different random_state, otherwise the folds will still be identical.\n",
        "\n",
        "# creating a StratifiedKFold classifier to train the models\n",
        "cv_tune = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# creating a StratifiedKFold classifier for Cross Validation\n",
        "cv_eval = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)"
      ],
      "metadata": {
        "id": "1CmjNCiud0N0"
      },
      "id": "1CmjNCiud0N0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GridSearchCV"
      ],
      "metadata": {
        "id": "7g8ajW7meUJ2"
      },
      "id": "7g8ajW7meUJ2"
    },
    {
      "cell_type": "code",
      "source": [
        "gs_lr = GridSearchCV(lr_pipe, lr_param_grid, cv = cv_tune, scoring='f1_weighted', n_jobs=-1)\n",
        "\n",
        "gs_rf = GridSearchCV(\n",
        "    rf_pipe,\n",
        "    rf_param_grid,\n",
        "    cv      =cv_tune,\n",
        "    scoring ='f1_weighted',\n",
        "    n_jobs  =-1,\n",
        "    verbose =1\n",
        ")"
      ],
      "metadata": {
        "id": "Wizld8UCeWao"
      },
      "id": "Wizld8UCeWao",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fit Models"
      ],
      "metadata": {
        "id": "fFFdnLmnoGRm"
      },
      "id": "fFFdnLmnoGRm"
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression model Fit\n",
        "gs_lr.fit(X_train, y_train)\n",
        "\n",
        "# Random Forest model Fit\n",
        "gs_rf.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "xEPMI9VnoKt7"
      },
      "id": "xEPMI9VnoKt7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Best Estimator"
      ],
      "metadata": {
        "id": "joUSOk71oH0C"
      },
      "id": "joUSOk71oH0C"
    },
    {
      "cell_type": "code",
      "source": [
        "best_lr = gs_lr.best_estimator_ # Best Logistic Regression Estimator\n",
        "\n",
        "best_rf = gs_rf.best_estimator_ # Best Random Forest Estimator"
      ],
      "metadata": {
        "id": "Z0PAgvl_oNZV"
      },
      "id": "Z0PAgvl_oNZV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cross Validation"
      ],
      "metadata": {
        "id": "gNf0KFXpeb44"
      },
      "id": "gNf0KFXpeb44"
    },
    {
      "cell_type": "code",
      "source": [
        "def run_cv(model_name, model, X_train, y_train, cv, scoring = None, return_train_score=False):\n",
        "  if scoring is None:\n",
        "    scoring = [\"accuracy\", \"f1_weighted\", \"roc_auc_ovr_weighted\"]\n",
        "  scores = cross_validate(model, X_train, y_train, cv=cv_eval, scoring=scoring, return_train_score=False)\n",
        "  return ({\n",
        "      \"Model\": model_name,\n",
        "      \"Accuracy Mean ± Std\": f'{np.mean(scores[\"test_accuracy\"]):.4f} ± {np.std(scores[\"test_accuracy\"], ddof=1):.4f}',\n",
        "      \"F1 Mean  ± Std\": f'{np.mean(scores[\"test_f1_weighted\"]):.4f} ± {np.std(scores[\"test_f1_weighted\"], ddof=1):.4f}',\n",
        "      \"AUC-ROC Mean  ± Std\": f'{np.mean(scores[\"test_roc_auc_ovr_weighted\"]):.4f} ± {np.std(scores[\"test_roc_auc_ovr_weighted\"], ddof=1):.4f}'\n",
        "  })"
      ],
      "metadata": {
        "id": "s9FQVr8FpY1f"
      },
      "id": "s9FQVr8FpY1f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores_lr = run_cv(\"Logistic\", best_lr, X_train, y_train, cv=cv_eval)\n",
        "\n",
        "print(\"----------------- Logistic Regression ---------------------------\")\n",
        "print(pd.DataFrame([scores_lr]))\n",
        "\n",
        "scores_rf = run_cv(\"Random Forest\", best_rf, X_train, y_train, cv=cv_eval)\n",
        "print(\"----------------- Random Forest -----------------------------------\")\n",
        "print(pd.DataFrame([scores_rf]))"
      ],
      "metadata": {
        "id": "BB-Zhgm9eeeh"
      },
      "id": "BB-Zhgm9eeeh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction"
      ],
      "metadata": {
        "id": "iyTcSnuEhwTl"
      },
      "id": "iyTcSnuEhwTl"
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions\n",
        "y_pred_lr = best_lr.predict(X_test)\n",
        "y_proba_lr = best_lr.predict_proba(X_test)\n",
        "\n",
        "y_pred_rf = best_rf.predict(X_test)\n",
        "y_proba_rf = best_rf.predict_proba(X_test)"
      ],
      "metadata": {
        "id": "5J3ZeVDWhzec"
      },
      "id": "5J3ZeVDWhzec",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accuracy, F1_score, roc_auc_score"
      ],
      "metadata": {
        "id": "FDCGOraahz3c"
      },
      "id": "FDCGOraahz3c"
    },
    {
      "cell_type": "code",
      "source": [
        "def print_acc_f1_roc(y_test, y_pred, y_proba):\n",
        "  # Accuracy\n",
        "  internal_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "  # F1 Score (weighted for multiclass safety)\n",
        "  internal_f1 = f1_score(y_test, y_pred, average='weighted', pos_label=1)\n",
        "\n",
        "  # ROC-AUC binary\n",
        "  internal_auc = roc_auc_score(y_test, y_proba[:, 1])\n",
        "\n",
        "  # Confusion Matrix\n",
        "  cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "  # Classification Report\n",
        "  report = classification_report(y_test, y_pred)\n",
        "\n",
        "  print(f\"Test Accuracy: {internal_accuracy:.4f}\")\n",
        "  print(f\"Test F1 Score: {internal_f1:.4f}\")\n",
        "  print(f\"Test ROC-AUC: {internal_auc:.4f}\\n\")\n",
        "\n",
        "  print(\"\\nClassification report:\\n\", report)\n",
        "\n",
        "  print(\"Confusion Matrix:\\n\", cm)\n",
        "\n",
        "  # return internal_accuracy, internal_f1, internal_auc\n"
      ],
      "metadata": {
        "id": "LZb46gN9rhcB"
      },
      "id": "LZb46gN9rhcB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_acc_f1_roc(y_test, y_pred_lr, y_proba_lr)\n",
        "\n",
        "\n",
        "print_acc_f1_roc(y_test, y_pred_rf, y_proba_rf)\n"
      ],
      "metadata": {
        "id": "C64_GKQwcgsc"
      },
      "id": "C64_GKQwcgsc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EXTERNAL TEST"
      ],
      "metadata": {
        "id": "BwJXle87jXWL"
      },
      "id": "BwJXle87jXWL"
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = df_test[num_features + cat_features + bin_features]\n",
        "y_test = df_test['y'].astype(int)"
      ],
      "metadata": {
        "id": "kc9Xq2U3l9y3"
      },
      "id": "kc9Xq2U3l9y3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions\n",
        "y_pred_lr = best_lr.predict(X_test)\n",
        "y_proba_lr = best_lr.predict_proba(X_test)\n",
        "\n",
        "y_pred_rf = best_rf.predict(X_test)\n",
        "y_proba_rf = best_rf.predict_proba(X_test)\n"
      ],
      "metadata": {
        "id": "JEg-aaLQjVra"
      },
      "id": "JEg-aaLQjVra",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_acc_f1_roc(y_test, y_pred_lr, y_proba_lr)\n",
        "\n",
        "print_acc_f1_roc(y_test, y_pred_rf, y_proba_rf)"
      ],
      "metadata": {
        "id": "5aMxy3sltMgG"
      },
      "id": "5aMxy3sltMgG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plotting ROC_Curve"
      ],
      "metadata": {
        "id": "ltZkM0MQwTF3"
      },
      "id": "ltZkM0MQwTF3"
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary of models\n",
        "models = {\n",
        "    \"Logistic Regression\": best_lr,\n",
        "    \"Random Forest\": best_rf\n",
        "}"
      ],
      "metadata": {
        "id": "IhAFaEldwbs5"
      },
      "id": "IhAFaEldwbs5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, model in models.items():\n",
        "    y_true_all = []\n",
        "    y_prob_all = []\n",
        "\n",
        "    for train_idx, test_idx in cv_eval.split(X, y):\n",
        "        est = clone(model)\n",
        "        est.fit(X.iloc[train_idx], y.iloc[train_idx])\n",
        "\n",
        "        # Predict probabilities for class 1\n",
        "        if hasattr(est, \"predict_proba\"):\n",
        "            y_prob = est.predict_proba(X.iloc[test_idx])[:, 1]\n",
        "        elif hasattr(est, \"decision_function\"):\n",
        "            y_prob = est.decision_function(X.iloc[test_idx])\n",
        "        else:\n",
        "            y_prob = est.predict(X.iloc[test_idx])  # fallback\n",
        "\n",
        "        y_true_all.append(y.iloc[test_idx])\n",
        "        y_prob_all.append(y_prob)\n",
        "\n",
        "    # Concatenate results from all folds\n",
        "    y_true_concat = np.concatenate(y_true_all)\n",
        "    y_prob_concat = np.concatenate(y_prob_all)\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(y_true_concat, y_prob_concat)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.plot(fpr, tpr, lw=2, label=f\"{name} (AUC = {roc_auc:.3f})\")\n",
        "\n",
        "\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curves for All Models')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DD87fjAvwSap"
      },
      "id": "DD87fjAvwSap",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hoan - GaussianNB"
      ],
      "metadata": {
        "id": "hD7uVaR0_9RM"
      },
      "id": "hD7uVaR0_9RM"
    },
    {
      "cell_type": "markdown",
      "source": [
        " For the Gaussian Naive Bayes model, feature scaling and one-hot encoding have already been completed, so the remaining step is to determine which features are most important. To accomplish this, the IAMB function is used to identify the features that contribute most effectively to the model.\n",
        "\n",
        "Gaussian Naive Bayes has very few hyperparameters, so GridSearch is unnecessary. Instead, 5-fold cross-validation is used on the training set to obtain an overall performance estimate. Afterward, the model is trained on the full training split and used to predict the test split. These test-split predictions are then compared with the predictions on the external test file.\n",
        "\n",
        "In this section, we compare the performance of a model trained on all available features with the performance of a model trained only on the IAMB-selected features to determine which approach yields better results."
      ],
      "metadata": {
        "id": "MWRxbHdxPmXS"
      },
      "id": "MWRxbHdxPmXS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Pingouin"
      ],
      "metadata": {
        "id": "13lNpaJrtwtx"
      },
      "id": "13lNpaJrtwtx"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pingouin"
      ],
      "metadata": {
        "id": "rd5NJEXqEdXg",
        "collapsed": true
      },
      "id": "rd5NJEXqEdXg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### IAMB\n"
      ],
      "metadata": {
        "id": "S5_Y9fqguFie"
      },
      "id": "S5_Y9fqguFie"
    },
    {
      "cell_type": "code",
      "source": [
        "# IAMB implementation with partial correlation\n",
        "# def iamb\n",
        "\n",
        "# Credited\n",
        "# https://dev.to/ogambakerubo/feature-selection-with-the-iamb-algorithm-a-casual-dive-into-machine-learning-53a7\n",
        "\n",
        "import pingouin as pg\n",
        "\n",
        "def iamb(data, target, alpha=0.05):\n",
        "\n",
        "  markov_blanket = set()\n",
        "\n",
        "  # Forward Phase\n",
        "  for feature in data.columns:\n",
        "    if feature != target:\n",
        "\n",
        "      # partial_corr\n",
        "      # ex:\n",
        "      #            n         r         CI95%         p-val\n",
        "      # pearson  768  0.221898  [0.15, 0.29]  5.065127e-10\n",
        "      #\n",
        "      # n     - number of valid samples used\n",
        "      # r     - partial correlations strength\n",
        "      # CI95% - 95% confidence interval for r\n",
        "      # p-val - Probability that the observed r occurred by chance if there's no real relationship\n",
        "\n",
        "      partial_corr  = pg.partial_corr(data=data, x=feature, y=target, covar=markov_blanket)\n",
        "      p_value       = partial_corr['p-val'].iloc[0]\n",
        "\n",
        "      # p-value > 0.05 - Could just be randome noise > Ignore feature\n",
        "      # p-value < 0.05 - Probably meaningful > Keep feature\n",
        "      if p_value < alpha:\n",
        "        markov_blanket.add(feature)\n",
        "\n",
        "  print(\"Selected Features after Forward Phase :\", markov_blanket)\n",
        "\n",
        "  # Backward Phase\n",
        "  for feature in list(markov_blanket):\n",
        "    reduced_mb    = markov_blanket - {feature}\n",
        "    partial_corr  = pg.partial_corr(data=data, x=feature, y=target, covar=reduced_mb)\n",
        "    p_value       = partial_corr['p-val'].iloc[0]\n",
        "    if p_value > alpha:\n",
        "      markov_blanket.remove(feature)\n",
        "\n",
        "  print(\"Selected Features after Backward Phase :\", markov_blanket)\n",
        "  return list(markov_blanket)"
      ],
      "metadata": {
        "id": "YEhD7GgIuEhd"
      },
      "id": "YEhD7GgIuEhd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Apply IAMB"
      ],
      "metadata": {
        "id": "NY7GuKP8uNpZ"
      },
      "id": "NY7GuKP8uNpZ"
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor_gnb.fit(gnb_X_train)\n",
        "\n",
        "# Transform\n",
        "X_train_proc  = preprocessor_gnb.transform(gnb_X_train)\n",
        "X_test_proc   = preprocessor_gnb.transform(gnb_X_test)\n",
        "\n",
        "feature_names = preprocessor_gnb.get_feature_names_out()\n",
        "\n",
        "# Convert to datafram\n",
        "X_train_proc_df = pd.DataFrame(\n",
        "  X_train_proc,\n",
        "  columns=feature_names,\n",
        "  index=gnb_X_train.index\n",
        ")\n",
        "\n",
        "X_test_proc_df = pd.DataFrame(\n",
        "  X_test_proc,\n",
        "  columns=feature_names,\n",
        "  index=gnb_X_test.index\n",
        ")\n",
        "\n",
        "# Adding 'y' back\n",
        "train_proc_with_y       = X_train_proc_df.copy()\n",
        "train_proc_with_y[\"y\"]  = gnb_y_train\n",
        "\n",
        "test_proc_with_y        = X_test_proc_df.copy()\n",
        "test_proc_with_y[\"y\"]   = gnb_y_test\n"
      ],
      "metadata": {
        "id": "BxufgrZWuRD3"
      },
      "id": "BxufgrZWuRD3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"From this many features:\", len(X_train_proc_df.columns))\n",
        "selected_features = iamb(train_proc_with_y, 'y', alpha=0.05)\n",
        "print(\"To this many features:\", len(selected_features))"
      ],
      "metadata": {
        "id": "2UT2VmMFuUWM"
      },
      "id": "2UT2VmMFuUWM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For readability\n",
        "gnb_X_train_2       = X_train_proc_df\n",
        "gnb_X_train_IAMB_2  = X_train_proc_df[selected_features]\n",
        "\n",
        "gnb_X_test_2        = X_test_proc_df\n",
        "gnb_X_test_IAMB_2   = X_test_proc_df[selected_features]"
      ],
      "metadata": {
        "id": "KyYZqS8ZuXju"
      },
      "id": "KyYZqS8ZuXju",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GNB Models"
      ],
      "metadata": {
        "id": "jmpz0OuxujH8"
      },
      "id": "jmpz0OuxujH8"
    },
    {
      "cell_type": "code",
      "source": [
        "gnb      = GaussianNB()  # All features\n",
        "gnb_iamb = GaussianNB()  # Selected features"
      ],
      "metadata": {
        "id": "LlR8RyqVumiH"
      },
      "id": "LlR8RyqVumiH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CV on TRAIN – GNB\n",
        "scores_gnb_train = run_cv(\"gnb\", gnb, gnb_X_train_2, gnb_y_train, cv=cv_eval)\n",
        "print(\"----------------- GNB 5 fold ---------------------------\")\n",
        "print(pd.DataFrame([scores_gnb_train]))\n",
        "print()\n",
        "\n",
        "# CV on TRAIN – GNB + IAMB\n",
        "scores_gnb_iamb_train = run_cv(\"gnb_iamb\", gnb_iamb, gnb_X_train_IAMB_2, gnb_y_train, cv=cv_eval)\n",
        "print(\"----------------- GNB IAMB 5 fold ---------------------------\")\n",
        "print(pd.DataFrame([scores_gnb_iamb_train]))\n",
        "print()\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ixPrnYHAwllI"
      },
      "id": "ixPrnYHAwllI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"----------------- GNB INTERNAL TEST SET EVALUATION ---------------------------\")\n",
        "# 1. Fit on full TRAIN\n",
        "gnb.fit(gnb_X_train_2, gnb_y_train)\n",
        "\n",
        "# 2. Predict on TEST\n",
        "y_pred_gnb  = gnb.predict(gnb_X_test_2)\n",
        "y_proba_gnb = gnb.predict_proba(gnb_X_test_2)  # <-- keep 2D\n",
        "\n",
        "print_acc_f1_roc(gnb_y_test, y_pred_gnb, y_proba_gnb)\n",
        "\n",
        "print(\"----------------- GNB IAMB INTERNAL TEST SET EVALUATION ---------------------------\")\n",
        "# 1. Fit on full TRAIN (IAMB-reduced)\n",
        "gnb_iamb.fit(gnb_X_train_IAMB_2, gnb_y_train)\n",
        "\n",
        "# 2. Predict on TEST (IAMB-reduced)\n",
        "y_pred_gnb_iamb  = gnb_iamb.predict(gnb_X_test_IAMB_2)\n",
        "y_proba_gnb_iamb = gnb_iamb.predict_proba(gnb_X_test_IAMB_2)  # <-- keep 2D\n",
        "\n",
        "print_acc_f1_roc(gnb_y_test, y_pred_gnb_iamb, y_proba_gnb_iamb)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "G0hLdkP1wmYd"
      },
      "id": "G0hLdkP1wmYd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### External Test"
      ],
      "metadata": {
        "id": "5HtQPCOoydjk"
      },
      "id": "5HtQPCOoydjk"
    },
    {
      "cell_type": "code",
      "source": [
        "gnb_EX = gnb_ex_df_test"
      ],
      "metadata": {
        "id": "HhPoWdvx34sS"
      },
      "id": "HhPoWdvx34sS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gnb_EX_x = gnb_EX.drop(columns=\"y\")\n",
        "gnb_EX_y = gnb_EX[\"y\"]"
      ],
      "metadata": {
        "id": "V15PwLbW4IWg"
      },
      "id": "V15PwLbW4IWg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Transform the new dataset (EX dataset)\n",
        "X_ex_proc = preprocessor_gnb.transform(gnb_EX_x)\n",
        "\n",
        "# 2. Convert to DataFrame with correct feature names\n",
        "X_ex_proc_df = pd.DataFrame(\n",
        "    X_ex_proc,\n",
        "    columns=feature_names,      # same names from training transform\n",
        "    index=gnb_EX_x.index\n",
        ")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "KwXObi-o4zV-"
      },
      "id": "KwXObi-o4zV-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For readability\n",
        "gnb_EX_x_set        = X_ex_proc_df\n",
        "gnb_EX_x_set_IAMB   = X_ex_proc_df[selected_features]"
      ],
      "metadata": {
        "id": "QuvZwWzc5MJf"
      },
      "id": "QuvZwWzc5MJf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"----------------- GNB EXTERNAL TEST SET EVALUATION ---------------------------\")\n",
        "gnb.fit(gnb_X_train_2, gnb_y_train)\n",
        "y_pred_gnb_ex  = gnb.predict(gnb_EX_x_set)\n",
        "y_proba_gnb_ex = gnb.predict_proba(gnb_EX_x_set)\n",
        "print_acc_f1_roc(gnb_EX_y, y_pred_gnb_ex, y_proba_gnb_ex)\n",
        "\n",
        "print(\"----------------- GNB IAMB EXTERNAL TEST SET EVALUATION ---------------------------\")\n",
        "gnb_iamb.fit(gnb_X_train_IAMB_2, gnb_y_train)\n",
        "y_pred_gnb_iamb_ex  = gnb_iamb.predict(gnb_EX_x_set_IAMB)\n",
        "y_proba_gnb_iamb_ex = gnb_iamb.predict_proba(gnb_EX_x_set_IAMB)\n",
        "print_acc_f1_roc(gnb_EX_y, y_pred_gnb_iamb_ex, y_proba_gnb_iamb_ex)"
      ],
      "metadata": {
        "id": "qMz4_Ntd5mGG"
      },
      "id": "qMz4_Ntd5mGG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Result"
      ],
      "metadata": {
        "id": "5Bq85OZnBnkT"
      },
      "id": "5Bq85OZnBnkT"
    },
    {
      "cell_type": "code",
      "source": [
        "results = [\n",
        "    [\"GNB (5-fold CV)\",               0.8442, 0.8498, 0.7388],\n",
        "    [\"GNB IAMB (5-fold CV)\",          0.8671, 0.8632, 0.7407],\n",
        "\n",
        "    [\"GNB (Internal Test)\",           0.8450, 0.8505, 0.7410],\n",
        "    [\"GNB IAMB (Internal Test)\",      0.8668, 0.8630, 0.7425],\n",
        "\n",
        "    [\"GNB (External Test)\",           0.8394, 0.8447, 0.7168],\n",
        "    [\"GNB IAMB (External Test)\",      0.8633, 0.8583, 0.7264],\n",
        "]\n",
        "\n",
        "df_results = pd.DataFrame(\n",
        "    results,\n",
        "    columns=[\"Model\", \"Accuracy\", \"F1 Score\", \"ROC-AUC\"]\n",
        ")\n",
        "\n",
        "display(df_results)\n"
      ],
      "metadata": {
        "id": "YJ4BogCABm5X"
      },
      "id": "YJ4BogCABm5X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the results, the IAMB-selected features consistently improve the Gaussian Naive Bayes model across all evaluation stages — cross-validation, the internal test split, and the external test file. The dataset likely contains correlated or noisy features (as seen in the heatmaps), which Gaussian Naive Bayes struggles with due to its independence assumption. By removing irrelevant and redundant features, IAMB reduces this noise, resulting in a cleaner and more informative feature set. As a result, the IAMB-enhanced model consistently achieves higher scores across all evaluation metrics."
      ],
      "metadata": {
        "id": "PqHlJsdnTArj"
      },
      "id": "PqHlJsdnTArj"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}