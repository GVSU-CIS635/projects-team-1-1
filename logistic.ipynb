{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GVSU-CIS635/projects-team-1-1/blob/main/logistic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing packages"
      ],
      "metadata": {
        "id": "Z1k6xILOGZ2s"
      },
      "id": "Z1k6xILOGZ2s"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "608743e5",
      "metadata": {
        "id": "608743e5",
        "outputId": "933020e6-a732-4c0e-8828-612c3d1a507d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.6.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.17.2)\n"
          ]
        }
      ],
      "source": [
        "# Installing Optuna first as it is an external dependency\n",
        "!pip install optuna\n",
        "\n",
        "# --- CONSOLIDATED IMPORTS ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import optuna\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# SKLEARN MODULES\n",
        "from sklearn.base import clone\n",
        "from sklearn.model_selection import (\n",
        "    train_test_split,\n",
        "    GridSearchCV,\n",
        "    StratifiedKFold,\n",
        "    StratifiedShuffleSplit,\n",
        "    cross_validate\n",
        ")\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import (\n",
        "    StandardScaler,\n",
        "    OneHotEncoder,\n",
        "    label_binarize # Although not used in final code, kept for completeness\n",
        ")\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# SKLEARN MODELS\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# SKLEARN METRICS\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    roc_auc_score,\n",
        "    classification_report,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    confusion_matrix,\n",
        "    precision_recall_curve, # Needed for threshold tuning\n",
        "    roc_curve,\n",
        "    auc\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Collection"
      ],
      "metadata": {
        "id": "QaOYRVCebyBd"
      },
      "id": "QaOYRVCebyBd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d5640aa",
      "metadata": {
        "id": "9d5640aa"
      },
      "outputs": [],
      "source": [
        "# Data Loading\n",
        "df_train = pd.read_csv(\"https://raw.githubusercontent.com/GVSU-CIS635/projects-team-1-1/main/data/train.csv\", sep=\";\", skipinitialspace=True)\n",
        "df_test = pd.read_csv(\"https://raw.githubusercontent.com/GVSU-CIS635/projects-team-1-1/main/data/test.csv\", sep=\";\", skipinitialspace=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Missing Values"
      ],
      "metadata": {
        "id": "g8W4jDBegCU9"
      },
      "id": "g8W4jDBegCU9"
    },
    {
      "cell_type": "code",
      "source": [
        "# looking for missing data\n",
        "print(\"Missing values found in Train\\n\", df_train.isnull().sum())\n",
        "\n",
        "# looking for duplicate data\n",
        "print(\"Duplicates found in Train \", df_train.duplicated().sum())\n",
        "\n",
        "print(\"---------------------------------------------------------\")\n",
        "# looking for missing data\n",
        "print(\"Missing values found in test\\n\", df_test.isnull().sum())\n",
        "\n",
        "# looking for duplicate data\n",
        "print(\"Duplicates found in test:\", df_test.duplicated().sum())"
      ],
      "metadata": {
        "id": "nRcjCCjUgDgd"
      },
      "id": "nRcjCCjUgDgd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even though the source has already stated that the data has no missing values or duplicates, it is still good to verify this, since these steps affect all the later processes."
      ],
      "metadata": {
        "id": "wt_gJR3IgO3j"
      },
      "id": "wt_gJR3IgO3j"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checking if Test is a subset of Train"
      ],
      "metadata": {
        "id": "Mmnm8TLHXjbf"
      },
      "id": "Mmnm8TLHXjbf"
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a boolean mask: for each test row, check if it appears in train\n",
        "mask = df_test.merge(df_train.drop_duplicates(), how='left', indicator=True)['_merge'] == 'both'"
      ],
      "metadata": {
        "id": "RzCNJ8FEXacO"
      },
      "id": "RzCNJ8FEXacO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_test = len(df_test)\n",
        "num_matches = mask.sum()\n",
        "num_missing = num_test - num_matches\n",
        "percent_match = num_matches / num_test * 100\n",
        "\n",
        "print(f\"Test rows: {num_test}\")\n",
        "print(f\"Rows that appear in train: {num_matches}\")\n",
        "print(f\"Rows NOT found in train: {num_missing}\")\n",
        "print(f\"Percent of test that is in train: {percent_match:.2f}%\")"
      ],
      "metadata": {
        "id": "OlmF0k4hXvz8"
      },
      "id": "OlmF0k4hXvz8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since test is a subset of train we can use training data to split into train(70%) and test(30%) and use it for models."
      ],
      "metadata": {
        "id": "0ONeWO_nX6aE"
      },
      "id": "0ONeWO_nX6aE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Group Classification"
      ],
      "metadata": {
        "id": "4Cq01kkbW3dt"
      },
      "id": "4Cq01kkbW3dt"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define feature groups\n",
        "num_features = [\"age\", \"balance\", \"day\", \"campaign\", \"pdays_numeric\", \"previous\"]\n",
        "cat_features = [\"job\", \"marital\", \"education\", \"contact\", \"month\", \"poutcome\"]\n",
        "bin_features = [\"default\", \"housing\", \"loan\"]"
      ],
      "metadata": {
        "id": "qTEV5MeWVJrr"
      },
      "id": "qTEV5MeWVJrr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Cleaning"
      ],
      "metadata": {
        "id": "YJm4t12MYNGr"
      },
      "id": "YJm4t12MYNGr"
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_external_minimal(df_ext: pd.DataFrame) -> pd.DataFrame:\n",
        "\n",
        "    # making a copy\n",
        "    dfx = df_ext.copy()\n",
        "\n",
        "    # Normalize headers\n",
        "    dfx.columns = dfx.columns.str.strip().str.lower()\n",
        "\n",
        "    if \"duration\" in dfx.columns:\n",
        "        dfx = dfx.drop(columns=[\"duration\"])\n",
        "\n",
        "    # Fix mixed-type categorical columns\n",
        "    cat_cols_train = df_train.select_dtypes(include=[\"object\"]).columns.tolist()\n",
        "    for c in cat_cols_train:\n",
        "        df_train[c] = df_train[c].astype(str)\n",
        "\n",
        "    dfx[\"contacted_before\"] = (dfx[\"pdays\"] != -1).astype(int)\n",
        "    dfx[\"pdays_numeric\"]    = dfx[\"pdays\"].replace(-1, 0)\n",
        "    for col in bin_features + [\"y\"]:\n",
        "        if col in dfx.columns:\n",
        "            dfx[col] = dfx[col].map({\"yes\": 1, \"no\": 0})\n",
        "    return dfx"
      ],
      "metadata": {
        "id": "v5OAEXuWfv0k"
      },
      "id": "v5OAEXuWfv0k",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = prepare_external_minimal(df_train)\n",
        "df_test  = prepare_external_minimal(df_test)"
      ],
      "metadata": {
        "id": "NmDnhO0sgf-G"
      },
      "id": "NmDnhO0sgf-G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 70% train / 30% internal test (keep separate test.csv untouched)\n",
        "X = df_train[num_features + cat_features + bin_features]\n",
        "y = df_train[\"y\"].astype(int)"
      ],
      "metadata": {
        "id": "gVFVXuyoaDvz"
      },
      "id": "gVFVXuyoaDvz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
        "\n",
        "for train_idx, test_idx in sss.split(X, y):\n",
        "  X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "  y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]"
      ],
      "metadata": {
        "id": "UOEpYEhMbjMt"
      },
      "id": "UOEpYEhMbjMt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "oKbSMovLaOqh"
      },
      "id": "oKbSMovLaOqh"
    },
    {
      "cell_type": "code",
      "source": [
        "# Imputer + Scaler are placed in every pipeline to avoid leakage and for consistent processing.\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "scaler  = StandardScaler()"
      ],
      "metadata": {
        "id": "L2419sQDuHuJ"
      },
      "id": "L2419sQDuHuJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_transformer = Pipeline(\n",
        "  steps=[\n",
        "  (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "  (\"scaler\", StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(\n",
        "  steps=[\n",
        "  (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "  (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
        "])"
      ],
      "metadata": {
        "id": "xg9t3GcPuwaS"
      },
      "id": "xg9t3GcPuwaS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing transformer\n",
        "preprocessor = ColumnTransformer(\n",
        "  transformers=[\n",
        "    ('num', numeric_transformer, num_features),\n",
        "    ('cat', categorical_transformer, cat_features),\n",
        "    ('bin', 'passthrough', bin_features),\n",
        "  ],\n",
        ")\n",
        "\n",
        "# HL\n",
        "preprocessor_gnb = ColumnTransformer(\n",
        "  transformers=[\n",
        "    ('num', numeric_transformer, num_features),\n",
        "    ('cat', categorical_transformer, cat_features),\n",
        "    ('bin', 'passthrough', bin_features),\n",
        "  ],\n",
        ")"
      ],
      "metadata": {
        "id": "b8z4JT5OaRMx"
      },
      "id": "b8z4JT5OaRMx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sri - Logistic Regression\n",
        "# San - Random Forest"
      ],
      "metadata": {
        "id": "JDKjhMgBUtio"
      },
      "id": "JDKjhMgBUtio"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipeline"
      ],
      "metadata": {
        "id": "F47R_2RNdAoO"
      },
      "id": "F47R_2RNdAoO"
    },
    {
      "cell_type": "code",
      "source": [
        "# creating pipelines\n",
        "lr_pipe = Pipeline([\n",
        "  ('preprocess', preprocessor),\n",
        "  ('clf', LogisticRegression(max_iter=2000))\n",
        "])"
      ],
      "metadata": {
        "id": "DPKe0KFZdCF9"
      },
      "id": "DPKe0KFZdCF9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parameter Grid"
      ],
      "metadata": {
        "id": "X8tcrqzIdFeH"
      },
      "id": "X8tcrqzIdFeH"
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters for Logistic Regression\n",
        "lr_param_grid = {\n",
        "  'clf__C': [0.01, 0.1, 1, 10, 100],\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Optional\n",
        "# print(\"\\n[Hyperparameter Grid]\")\n",
        "# for param, values in rf_param_grid.items():\n",
        "#     print(f\"  {param}: {values}\")\n",
        "# print(f\"Total combinations: {len(rf_param_grid['clf__n_estimators']) * len(rf_param_grid['clf__max_depth']) * len(rf_param_grid['clf__min_samples_split'])}\")\n"
      ],
      "metadata": {
        "id": "7JeLa0TFdIpv"
      },
      "id": "7JeLa0TFdIpv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## StratifiedKFold"
      ],
      "metadata": {
        "id": "O4jAfY2ddqM7"
      },
      "id": "O4jAfY2ddqM7"
    },
    {
      "cell_type": "code",
      "source": [
        "# creating 2 seperate because this avoids optimistic bias because you evaluate on folds that the model has not seen during hyperparameter tuning.\n",
        "# use the different random_state, otherwise the folds will still be identical.\n",
        "\n",
        "# creating a StratifiedKFold classifier to train the models\n",
        "cv_tune = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# creating a StratifiedKFold classifier for Cross Validation\n",
        "cv_eval = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)"
      ],
      "metadata": {
        "id": "1CmjNCiud0N0"
      },
      "id": "1CmjNCiud0N0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GridSearchCV"
      ],
      "metadata": {
        "id": "7g8ajW7meUJ2"
      },
      "id": "7g8ajW7meUJ2"
    },
    {
      "cell_type": "code",
      "source": [
        "gs_lr = GridSearchCV(lr_pipe, lr_param_grid, cv = cv_tune, scoring='f1_weighted', n_jobs=-1)\n"
      ],
      "metadata": {
        "id": "Wizld8UCeWao"
      },
      "id": "Wizld8UCeWao",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fit Models"
      ],
      "metadata": {
        "id": "fFFdnLmnoGRm"
      },
      "id": "fFFdnLmnoGRm"
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression model Fit\n",
        "gs_lr.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "xEPMI9VnoKt7"
      },
      "id": "xEPMI9VnoKt7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Best Estimator"
      ],
      "metadata": {
        "id": "joUSOk71oH0C"
      },
      "id": "joUSOk71oH0C"
    },
    {
      "cell_type": "code",
      "source": [
        "best_lr = gs_lr.best_estimator_ # Best Logistic Regression Estimator\n"
      ],
      "metadata": {
        "id": "Z0PAgvl_oNZV"
      },
      "id": "Z0PAgvl_oNZV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cross Validation"
      ],
      "metadata": {
        "id": "gNf0KFXpeb44"
      },
      "id": "gNf0KFXpeb44"
    },
    {
      "cell_type": "code",
      "source": [
        "def run_cv(model_name, model, X_train, y_train, cv, scoring = None, return_train_score=False):\n",
        "  if scoring is None:\n",
        "    scoring = [\"accuracy\", \"f1_weighted\", \"roc_auc_ovr_weighted\"]\n",
        "  scores = cross_validate(model, X_train, y_train, cv=cv_eval, scoring=scoring, return_train_score=False)\n",
        "  return ({\n",
        "      \"Model\": model_name,\n",
        "      \"Accuracy Mean ± Std\": f'{np.mean(scores[\"test_accuracy\"]):.4f} ± {np.std(scores[\"test_accuracy\"], ddof=1):.4f}',\n",
        "      \"F1 Mean  ± Std\": f'{np.mean(scores[\"test_f1_weighted\"]):.4f} ± {np.std(scores[\"test_f1_weighted\"], ddof=1):.4f}',\n",
        "      \"AUC-ROC Mean  ± Std\": f'{np.mean(scores[\"test_roc_auc_ovr_weighted\"]):.4f} ± {np.std(scores[\"test_roc_auc_ovr_weighted\"], ddof=1):.4f}'\n",
        "  })"
      ],
      "metadata": {
        "id": "s9FQVr8FpY1f"
      },
      "id": "s9FQVr8FpY1f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores_lr = run_cv(\"Logistic\", best_lr, X_train, y_train, cv=cv_eval)\n",
        "\n",
        "print(\"----------------- Logistic Regression ---------------------------\")\n",
        "print(pd.DataFrame([scores_lr]))\n",
        "\n",
        "scores_rf = run_cv(\"Random Forest\", best_rf, X_train, y_train, cv=cv_eval)\n",
        "print(\"----------------- Random Forest -----------------------------------\")\n",
        "print(pd.DataFrame([scores_rf]))"
      ],
      "metadata": {
        "id": "BB-Zhgm9eeeh"
      },
      "id": "BB-Zhgm9eeeh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction"
      ],
      "metadata": {
        "id": "iyTcSnuEhwTl"
      },
      "id": "iyTcSnuEhwTl"
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions\n",
        "y_pred_lr = best_lr.predict(X_test)\n",
        "y_proba_lr = best_lr.predict_proba(X_test)\n"
      ],
      "metadata": {
        "id": "5J3ZeVDWhzec"
      },
      "id": "5J3ZeVDWhzec",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accuracy, F1_score, roc_auc_score"
      ],
      "metadata": {
        "id": "FDCGOraahz3c"
      },
      "id": "FDCGOraahz3c"
    },
    {
      "cell_type": "code",
      "source": [
        "def print_acc_f1_roc(y_test, y_pred, y_proba):\n",
        "  # Accuracy\n",
        "  internal_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "  # F1 Score (weighted for multiclass safety)\n",
        "  internal_f1 = f1_score(y_test, y_pred, average='weighted', pos_label=1)\n",
        "\n",
        "  # ROC-AUC binary\n",
        "  internal_auc = roc_auc_score(y_test, y_proba[:, 1])\n",
        "\n",
        "  # Confusion Matrix\n",
        "  cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "  # Classification Report\n",
        "  report = classification_report(y_test, y_pred)\n",
        "\n",
        "  print(f\"Test Accuracy: {internal_accuracy:.4f}\")\n",
        "  print(f\"Test F1 Score: {internal_f1:.4f}\")\n",
        "  print(f\"Test ROC-AUC: {internal_auc:.4f}\\n\")\n",
        "\n",
        "  print(\"\\nClassification report:\\n\", report)\n",
        "\n",
        "  print(\"Confusion Matrix:\\n\", cm)\n",
        "\n",
        "  # return internal_accuracy, internal_f1, internal_auc\n"
      ],
      "metadata": {
        "id": "LZb46gN9rhcB"
      },
      "id": "LZb46gN9rhcB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_acc_f1_roc(y_test, y_pred_lr, y_proba_lr)"
      ],
      "metadata": {
        "id": "C64_GKQwcgsc"
      },
      "id": "C64_GKQwcgsc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EXTERNAL TEST"
      ],
      "metadata": {
        "id": "BwJXle87jXWL"
      },
      "id": "BwJXle87jXWL"
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = df_test[num_features + cat_features + bin_features]\n",
        "y_test = df_test['y'].astype(int)"
      ],
      "metadata": {
        "id": "kc9Xq2U3l9y3"
      },
      "id": "kc9Xq2U3l9y3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions\n",
        "y_pred_lr = best_lr.predict(X_test)\n",
        "y_proba_lr = best_lr.predict_proba(X_test)\n"
      ],
      "metadata": {
        "id": "JEg-aaLQjVra"
      },
      "id": "JEg-aaLQjVra",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_acc_f1_roc(y_test, y_pred_lr, y_proba_lr)"
      ],
      "metadata": {
        "id": "5aMxy3sltMgG"
      },
      "id": "5aMxy3sltMgG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plotting ROC_Curve"
      ],
      "metadata": {
        "id": "ltZkM0MQwTF3"
      },
      "id": "ltZkM0MQwTF3"
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary of models\n",
        "models = {\n",
        "    \"Logistic Regression\": best_lr\n",
        "}"
      ],
      "metadata": {
        "id": "IhAFaEldwbs5"
      },
      "id": "IhAFaEldwbs5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, model in models.items():\n",
        "    y_true_all = []\n",
        "    y_prob_all = []\n",
        "\n",
        "    for train_idx, test_idx in cv_eval.split(X, y):\n",
        "        est = clone(model)\n",
        "        est.fit(X.iloc[train_idx], y.iloc[train_idx])\n",
        "\n",
        "        # Predict probabilities for class 1\n",
        "        if hasattr(est, \"predict_proba\"):\n",
        "            y_prob = est.predict_proba(X.iloc[test_idx])[:, 1]\n",
        "        elif hasattr(est, \"decision_function\"):\n",
        "            y_prob = est.decision_function(X.iloc[test_idx])\n",
        "        else:\n",
        "            y_prob = est.predict(X.iloc[test_idx])  # fallback\n",
        "\n",
        "        y_true_all.append(y.iloc[test_idx])\n",
        "        y_prob_all.append(y_prob)\n",
        "\n",
        "    # Concatenate results from all folds\n",
        "    y_true_concat = np.concatenate(y_true_all)\n",
        "    y_prob_concat = np.concatenate(y_prob_all)\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(y_true_concat, y_prob_concat)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.plot(fpr, tpr, lw=2, label=f\"{name} (AUC = {roc_auc:.3f})\")\n",
        "\n",
        "\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curves for All Models')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DD87fjAvwSap"
      },
      "id": "DD87fjAvwSap",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}