{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GVSU-CIS635/projects-team-1-1/blob/main/logistic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing packages"
      ],
      "metadata": {
        "id": "Z1k6xILOGZ2s"
      },
      "id": "Z1k6xILOGZ2s"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "608743e5",
      "metadata": {
        "id": "608743e5",
        "outputId": "69836807-2e2a-4a5d-b545-efeeaa968cd4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optuna in /usr/local/lib/python3.12/dist-packages (4.6.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.17.2)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.12/dist-packages (from optuna) (6.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.44)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.3)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "# Installing Optuna first as it is an external dependency\n",
        "!pip install optuna\n",
        "\n",
        "# --- CONSOLIDATED IMPORTS ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import optuna\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# SKLEARN MODULES\n",
        "from sklearn.base import clone\n",
        "from sklearn.model_selection import (\n",
        "    train_test_split,\n",
        "    GridSearchCV,\n",
        "    StratifiedKFold,\n",
        "    StratifiedShuffleSplit,\n",
        "    cross_validate\n",
        ")\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import (\n",
        "    StandardScaler,\n",
        "    OneHotEncoder,\n",
        "    label_binarize # Although not used in final code, kept for completeness\n",
        ")\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# SKLEARN MODELS\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# SKLEARN METRICS\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    roc_auc_score,\n",
        "    classification_report,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    confusion_matrix,\n",
        "    precision_recall_curve, # Needed for threshold tuning\n",
        "    roc_curve,\n",
        "    auc\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Collection"
      ],
      "metadata": {
        "id": "QaOYRVCebyBd"
      },
      "id": "QaOYRVCebyBd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d5640aa",
      "metadata": {
        "id": "9d5640aa"
      },
      "outputs": [],
      "source": [
        "# Data Loading\n",
        "df_train = pd.read_csv(\"https://raw.githubusercontent.com/GVSU-CIS635/projects-team-1-1/main/data/train.csv\", sep=\";\", skipinitialspace=True)\n",
        "df_test = pd.read_csv(\"https://raw.githubusercontent.com/GVSU-CIS635/projects-team-1-1/main/data/test.csv\", sep=\";\", skipinitialspace=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Missing Values"
      ],
      "metadata": {
        "id": "g8W4jDBegCU9"
      },
      "id": "g8W4jDBegCU9"
    },
    {
      "cell_type": "code",
      "source": [
        "# looking for missing data\n",
        "print(\"Missing values found in Train\\n\", df_train.isnull().sum())\n",
        "\n",
        "# looking for duplicate data\n",
        "print(\"Duplicates found in Train \", df_train.duplicated().sum())\n",
        "\n",
        "print(\"---------------------------------------------------------\")\n",
        "# looking for missing data\n",
        "print(\"Missing values found in test\\n\", df_test.isnull().sum())\n",
        "\n",
        "# looking for duplicate data\n",
        "print(\"Duplicates found in test:\", df_test.duplicated().sum())"
      ],
      "metadata": {
        "id": "nRcjCCjUgDgd"
      },
      "id": "nRcjCCjUgDgd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even though the source has already stated that the data has no missing values or duplicates, it is still good to verify this, since these steps affect all the later processes."
      ],
      "metadata": {
        "id": "wt_gJR3IgO3j"
      },
      "id": "wt_gJR3IgO3j"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checking if Test is a subset of Train"
      ],
      "metadata": {
        "id": "Mmnm8TLHXjbf"
      },
      "id": "Mmnm8TLHXjbf"
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a boolean mask: for each test row, check if it appears in train\n",
        "mask = df_test.merge(df_train.drop_duplicates(), how='left', indicator=True)['_merge'] == 'both'"
      ],
      "metadata": {
        "id": "RzCNJ8FEXacO"
      },
      "id": "RzCNJ8FEXacO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_test = len(df_test)\n",
        "num_matches = mask.sum()\n",
        "num_missing = num_test - num_matches\n",
        "percent_match = num_matches / num_test * 100\n",
        "\n",
        "print(f\"Test rows: {num_test}\")\n",
        "print(f\"Rows that appear in train: {num_matches}\")\n",
        "print(f\"Rows NOT found in train: {num_missing}\")\n",
        "print(f\"Percent of test that is in train: {percent_match:.2f}%\")"
      ],
      "metadata": {
        "id": "OlmF0k4hXvz8"
      },
      "id": "OlmF0k4hXvz8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since test is a subset of train we can use training data to split into train(70%) and test(30%) and use it for models."
      ],
      "metadata": {
        "id": "0ONeWO_nX6aE"
      },
      "id": "0ONeWO_nX6aE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Group Classification"
      ],
      "metadata": {
        "id": "4Cq01kkbW3dt"
      },
      "id": "4Cq01kkbW3dt"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define feature groups\n",
        "num_features = [\"age\", \"balance\", \"day\", \"campaign\", \"pdays_numeric\", \"previous\"]\n",
        "cat_features = [\"job\", \"marital\", \"education\", \"contact\", \"month\", \"poutcome\"]\n",
        "bin_features = [\"default\", \"housing\", \"loan\"]"
      ],
      "metadata": {
        "id": "qTEV5MeWVJrr"
      },
      "id": "qTEV5MeWVJrr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Cleaning"
      ],
      "metadata": {
        "id": "YJm4t12MYNGr"
      },
      "id": "YJm4t12MYNGr"
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_external_minimal(df_ext: pd.DataFrame) -> pd.DataFrame:\n",
        "\n",
        "    # making a copy\n",
        "    dfx = df_ext.copy()\n",
        "\n",
        "    # Normalize headers\n",
        "    dfx.columns = dfx.columns.str.strip().str.lower()\n",
        "\n",
        "    if \"duration\" in dfx.columns:\n",
        "        dfx = dfx.drop(columns=[\"duration\"])\n",
        "\n",
        "    # Fix mixed-type categorical columns\n",
        "    cat_cols_train = df_train.select_dtypes(include=[\"object\"]).columns.tolist()\n",
        "    for c in cat_cols_train:\n",
        "        df_train[c] = df_train[c].astype(str)\n",
        "\n",
        "    dfx[\"contacted_before\"] = (dfx[\"pdays\"] != -1).astype(int)\n",
        "    dfx[\"pdays_numeric\"]    = dfx[\"pdays\"].replace(-1, 0)\n",
        "    for col in bin_features + [\"y\"]:\n",
        "        if col in dfx.columns:\n",
        "            dfx[col] = dfx[col].map({\"yes\": 1, \"no\": 0})\n",
        "    return dfx"
      ],
      "metadata": {
        "id": "v5OAEXuWfv0k"
      },
      "id": "v5OAEXuWfv0k",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = prepare_external_minimal(df_train)\n",
        "df_test  = prepare_external_minimal(df_test)"
      ],
      "metadata": {
        "id": "NmDnhO0sgf-G"
      },
      "id": "NmDnhO0sgf-G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HL\n",
        "gnb_ex_df_test = df_test.copy()"
      ],
      "metadata": {
        "id": "Syt-80KI9_SH"
      },
      "id": "Syt-80KI9_SH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e2c29ce0",
      "metadata": {
        "id": "e2c29ce0"
      },
      "source": [
        "# Numeric Features Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e433947",
      "metadata": {
        "id": "3e433947"
      },
      "outputs": [],
      "source": [
        "# Box plot\n",
        "plt.figure(figsize=(12,8))\n",
        "for i, col in enumerate(num_features, 1):\n",
        "    plt.subplot(3, 3, i)\n",
        "    sns.boxplot(x = df_train[col], color=\"skyblue\")\n",
        "    plt.title(f\"{col}\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3041529",
      "metadata": {
        "id": "f3041529"
      },
      "outputs": [],
      "source": [
        "# Distribution\n",
        "plt.figure(figsize=(12,8))\n",
        "for i, col in enumerate(num_features, 1):\n",
        "    plt.subplot(3, 3, i)\n",
        "    sns.histplot(df_train[col], bins=30, kde=True)\n",
        "    plt.title(f\"{col}\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f61abbb",
      "metadata": {
        "id": "9f61abbb"
      },
      "outputs": [],
      "source": [
        "# Correlation\n",
        "corr = df_train[num_features].corr()\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title(\"Correlation Heatmap (Numeric Features)\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "946909b3",
      "metadata": {
        "id": "946909b3"
      },
      "source": [
        "We are trying to understand the numerical features in this section:\n",
        "- How are the features distributed? Do they show any skewness?\n",
        "- What are the correlations among the features?\n",
        "- This will help us make better judgments on whether remove outliers, normalize, or remove similar features.\n",
        "\n",
        "From the box plot and the distribution graphs\n",
        "- Age       : that looks ok, with outliers\n",
        "- Balance   : most people fill in 0, maybe people over look this section\n",
        "- Day       : no outliers, good distribution\n",
        "- Duration  : dropped due to data leakage\n",
        "- Campaign  : more than 6 calls during a campain and that would be too many call for one personm > skewed right\n",
        "- Pdays = contacted_before + pdays_numeric bc -1 means client was not previously contacted, there are too many -1 (81.7%) > skewed right\n",
        "- Previous  : most clients are new (81.7%) > skewed right\n",
        "\n",
        "Notes:\n",
        "- From the correlation heatmap, most features are not correlated to anothers so we can keep them all\n",
        "\n",
        "Actions\n",
        "- Age           : StandardScaler\n",
        "- Balance       : Skewed right with negative values > StandardScaler\n",
        "- Day           : StandardScaler\n",
        "- Campaign      : log1p transformed > StandardScaler\n",
        "- pdays_numeric : log1p transformed > StandardScaler\n",
        "- previous      : log1p transformed > StandardScaler\n",
        "\n",
        "What is log1p transform?\n",
        "- Reduces skewness\n",
        "- Makes the distribution more normal-like\n",
        "- Helps with logistic regression"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11963bac",
      "metadata": {
        "id": "11963bac"
      },
      "source": [
        "# Categorical features Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64918ab6",
      "metadata": {
        "id": "64918ab6"
      },
      "outputs": [],
      "source": [
        "# Graphs\n",
        "# Number of plots\n",
        "n     = len(cat_features)         # 10\n",
        "ncols = 3                         # 3 columns\n",
        "nrows = n // ncols + 1            # 10 // 3 + 1 = 4\n",
        "\n",
        "plt.figure(figsize=(5*ncols, 4*nrows))\n",
        "\n",
        "for i, col in enumerate(cat_features, 1):\n",
        "    plt.subplot(nrows, ncols, i)\n",
        "    ax = sns.countplot(\n",
        "        x=col,\n",
        "        hue=col,\n",
        "        data=df_train,\n",
        "        palette='pastel',\n",
        "        order=df_train[col].value_counts().index,\n",
        "        legend=False\n",
        "    )\n",
        "\n",
        "    total = len(df_train)\n",
        "\n",
        "    # find tallest bar to give extra y-axis space\n",
        "    max_height = max(p.get_height() for p in ax.patches)\n",
        "    ax.set_ylim(0, max_height * 1.10)  # 15% space above bars\n",
        "\n",
        "    # annotate each bar\n",
        "    for p in ax.patches:\n",
        "        count = p.get_height()\n",
        "        percentage = 100 * count / total\n",
        "\n",
        "        # annotate text slightly above bar top\n",
        "        ax.annotate(\n",
        "            f'{percentage:.1f}%',\n",
        "            xy=(p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "            xytext=(0, 6),                  # 6 points above the bar\n",
        "            textcoords='offset points',\n",
        "            ha='center', va='bottom',\n",
        "            fontsize=9, color='black'\n",
        "        )\n",
        "\n",
        "    plt.title(col.capitalize(), fontsize=11)\n",
        "    plt.xlabel('')\n",
        "    plt.ylabel('')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5aab569b",
      "metadata": {
        "id": "5aab569b"
      },
      "source": [
        "From the bar charts:\n",
        "- There are some dominant responses among the features; during training, the classes(freatures) will therefore be weighted.\n",
        "\n",
        "What is Cramér’s V correlation coefficient?\n",
        "- It measures the strength of association between two categorical variables.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "feb767b8",
      "metadata": {
        "id": "feb767b8"
      },
      "outputs": [],
      "source": [
        "def cramers_v(x, y):\n",
        "    table = pd.crosstab(x, y)\n",
        "    chi2  = chi2_contingency(table, correction = False)[0]\n",
        "    n     = table.sum().sum()\n",
        "    k     = min(table.shape)\n",
        "    return np.sqrt(chi2 / (n * (k - 1)))\n",
        "\n",
        "# build the matrix\n",
        "cramers = pd.DataFrame(index = cat_features, columns=cat_features, dtype=float)\n",
        "\n",
        "for c1 in cat_features:\n",
        "    for c2 in cat_features:\n",
        "        cramers.loc[c1, c2] = cramers_v(df_train[c1], df_train[c2])\n",
        "\n",
        "# visualize\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cramers.astype(float), annot=True, cmap='Blues', fmt=\".2f\")\n",
        "plt.title(\"Cramér's V Correlation between Categorical Features\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29e94cee",
      "metadata": {
        "id": "29e94cee"
      },
      "source": [
        "From the Cramer's V heat map\n",
        "- There is no strong correlation among features\n",
        "- 0.5 and 0.46 are the two most significant values\n",
        "- education and job are kind of related - if you are in school > you are a student\n",
        "- month and housing - in the summer, people are just happy to go buy a house\n",
        "- month and contact - in the summer, people are just happy to pickup the phone"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train/Test split"
      ],
      "metadata": {
        "id": "gmKxqtBAaHe4"
      },
      "id": "gmKxqtBAaHe4"
    },
    {
      "cell_type": "code",
      "source": [
        "# 70% train / 30% internal test (keep separate test.csv untouched)\n",
        "X = df_train[num_features + cat_features + bin_features]\n",
        "y = df_train[\"y\"].astype(int)"
      ],
      "metadata": {
        "id": "gVFVXuyoaDvz"
      },
      "id": "gVFVXuyoaDvz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
        "\n",
        "for train_idx, test_idx in sss.split(X, y):\n",
        "  X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "  y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]"
      ],
      "metadata": {
        "id": "UOEpYEhMbjMt"
      },
      "id": "UOEpYEhMbjMt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HL\n",
        "gnb_X_train = X_train.copy()\n",
        "gnb_X_test  = X_train.copy()\n",
        "gnb_y_train = y_train.copy()\n",
        "gnb_y_test  = y_train.copy()"
      ],
      "metadata": {
        "id": "6mZVegy3RjUq"
      },
      "id": "6mZVegy3RjUq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "oKbSMovLaOqh"
      },
      "id": "oKbSMovLaOqh"
    },
    {
      "cell_type": "code",
      "source": [
        "# Imputer + Scaler are placed in every pipeline to avoid leakage and for consistent processing.\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "scaler  = StandardScaler()"
      ],
      "metadata": {
        "id": "L2419sQDuHuJ"
      },
      "id": "L2419sQDuHuJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_transformer = Pipeline(\n",
        "  steps=[\n",
        "  (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "  (\"scaler\", StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(\n",
        "  steps=[\n",
        "  (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "  (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
        "])"
      ],
      "metadata": {
        "id": "xg9t3GcPuwaS"
      },
      "id": "xg9t3GcPuwaS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing transformer\n",
        "preprocessor = ColumnTransformer(\n",
        "  transformers=[\n",
        "    ('num', numeric_transformer, num_features),\n",
        "    ('cat', categorical_transformer, cat_features),\n",
        "    ('bin', 'passthrough', bin_features),\n",
        "  ],\n",
        ")\n",
        "\n",
        "# HL\n",
        "preprocessor_gnb = ColumnTransformer(\n",
        "  transformers=[\n",
        "    ('num', numeric_transformer, num_features),\n",
        "    ('cat', categorical_transformer, cat_features),\n",
        "    ('bin', 'passthrough', bin_features),\n",
        "  ],\n",
        ")"
      ],
      "metadata": {
        "id": "b8z4JT5OaRMx"
      },
      "id": "b8z4JT5OaRMx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### After the proprocessing, everyone can do what ever they want"
      ],
      "metadata": {
        "id": "elUmgcsMANwE"
      },
      "id": "elUmgcsMANwE"
    },
    {
      "cell_type": "code",
      "source": [
        "# train.csv > train_set + test_set (70/30) use shuffle\n",
        "\n",
        "# 1. internal testing for the test_set\n",
        "\n",
        "# 2. external testing using test.csv\n",
        "\n",
        "# Compare 1 vs 2"
      ],
      "metadata": {
        "id": "cVKCQMXNAZJF"
      },
      "id": "cVKCQMXNAZJF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sri - Logistic Regression\n",
        "# San - Random Forest"
      ],
      "metadata": {
        "id": "JDKjhMgBUtio"
      },
      "id": "JDKjhMgBUtio"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipeline"
      ],
      "metadata": {
        "id": "F47R_2RNdAoO"
      },
      "id": "F47R_2RNdAoO"
    },
    {
      "cell_type": "code",
      "source": [
        "# creating pipelines\n",
        "lr_pipe = Pipeline([\n",
        "  ('preprocess', preprocessor),\n",
        "  ('clf', LogisticRegression(max_iter=2000))\n",
        "])"
      ],
      "metadata": {
        "id": "DPKe0KFZdCF9"
      },
      "id": "DPKe0KFZdCF9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parameter Grid"
      ],
      "metadata": {
        "id": "X8tcrqzIdFeH"
      },
      "id": "X8tcrqzIdFeH"
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters for Logistic Regression\n",
        "lr_param_grid = {\n",
        "  'clf__C': [0.01, 0.1, 1, 10, 100],\n",
        "}\n",
        "\n",
        "\n",
        "# Optional\n",
        "# print(\"\\n[Hyperparameter Grid]\")\n",
        "# for param, values in rf_param_grid.items():\n",
        "#     print(f\"  {param}: {values}\")\n",
        "# print(f\"Total combinations: {len(rf_param_grid['clf__n_estimators']) * len(rf_param_grid['clf__max_depth']) * len(rf_param_grid['clf__min_samples_split'])}\")\n"
      ],
      "metadata": {
        "id": "7JeLa0TFdIpv"
      },
      "id": "7JeLa0TFdIpv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## StratifiedKFold"
      ],
      "metadata": {
        "id": "O4jAfY2ddqM7"
      },
      "id": "O4jAfY2ddqM7"
    },
    {
      "cell_type": "code",
      "source": [
        "# creating 2 seperate because this avoids optimistic bias because you evaluate on folds that the model has not seen during hyperparameter tuning.\n",
        "# use the different random_state, otherwise the folds will still be identical.\n",
        "\n",
        "# creating a StratifiedKFold classifier to train the models\n",
        "cv_tune = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# creating a StratifiedKFold classifier for Cross Validation\n",
        "cv_eval = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)"
      ],
      "metadata": {
        "id": "1CmjNCiud0N0"
      },
      "id": "1CmjNCiud0N0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GridSearchCV"
      ],
      "metadata": {
        "id": "7g8ajW7meUJ2"
      },
      "id": "7g8ajW7meUJ2"
    },
    {
      "cell_type": "code",
      "source": [
        "gs_lr = GridSearchCV(lr_pipe, lr_param_grid, cv = cv_tune, scoring='f1_weighted', n_jobs=-1)\n"
      ],
      "metadata": {
        "id": "Wizld8UCeWao"
      },
      "id": "Wizld8UCeWao",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fit Models"
      ],
      "metadata": {
        "id": "fFFdnLmnoGRm"
      },
      "id": "fFFdnLmnoGRm"
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression model Fit\n",
        "gs_lr.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "xEPMI9VnoKt7"
      },
      "id": "xEPMI9VnoKt7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Best Estimator"
      ],
      "metadata": {
        "id": "joUSOk71oH0C"
      },
      "id": "joUSOk71oH0C"
    },
    {
      "cell_type": "code",
      "source": [
        "best_lr = gs_lr.best_estimator_ # Best Logistic Regression Estimator"
      ],
      "metadata": {
        "id": "Z0PAgvl_oNZV"
      },
      "id": "Z0PAgvl_oNZV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cross Validation"
      ],
      "metadata": {
        "id": "gNf0KFXpeb44"
      },
      "id": "gNf0KFXpeb44"
    },
    {
      "cell_type": "code",
      "source": [
        "def run_cv(model_name, model, X_train, y_train, cv, scoring = None, return_train_score=False):\n",
        "  if scoring is None:\n",
        "    scoring = [\"accuracy\", \"f1_weighted\", \"roc_auc_ovr_weighted\"]\n",
        "  scores = cross_validate(model, X_train, y_train, cv=cv_eval, scoring=scoring, return_train_score=False)\n",
        "  return ({\n",
        "      \"Model\": model_name,\n",
        "      \"Accuracy Mean ± Std\": f'{np.mean(scores[\"test_accuracy\"]):.4f} ± {np.std(scores[\"test_accuracy\"], ddof=1):.4f}',\n",
        "      \"F1 Mean  ± Std\": f'{np.mean(scores[\"test_f1_weighted\"]):.4f} ± {np.std(scores[\"test_f1_weighted\"], ddof=1):.4f}',\n",
        "      \"AUC-ROC Mean  ± Std\": f'{np.mean(scores[\"test_roc_auc_ovr_weighted\"]):.4f} ± {np.std(scores[\"test_roc_auc_ovr_weighted\"], ddof=1):.4f}'\n",
        "  })"
      ],
      "metadata": {
        "id": "s9FQVr8FpY1f"
      },
      "id": "s9FQVr8FpY1f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores_lr = run_cv(\"Logistic\", best_lr, X_train, y_train, cv=cv_eval)\n",
        "\n",
        "print(\"----------------- Logistic Regression ---------------------------\")\n",
        "print(pd.DataFrame([scores_lr]))"
      ],
      "metadata": {
        "id": "BB-Zhgm9eeeh"
      },
      "id": "BB-Zhgm9eeeh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction"
      ],
      "metadata": {
        "id": "iyTcSnuEhwTl"
      },
      "id": "iyTcSnuEhwTl"
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions\n",
        "y_pred_lr = best_lr.predict(X_test)\n",
        "y_proba_lr = best_lr.predict_proba(X_test)\n"
      ],
      "metadata": {
        "id": "5J3ZeVDWhzec"
      },
      "id": "5J3ZeVDWhzec",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accuracy, F1_score, roc_auc_score"
      ],
      "metadata": {
        "id": "FDCGOraahz3c"
      },
      "id": "FDCGOraahz3c"
    },
    {
      "cell_type": "code",
      "source": [
        "def print_acc_f1_roc(y_test, y_pred, y_proba):\n",
        "  # Accuracy\n",
        "  internal_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "  # F1 Score (weighted for multiclass safety)\n",
        "  internal_f1 = f1_score(y_test, y_pred, average='weighted', pos_label=1)\n",
        "\n",
        "  # ROC-AUC binary\n",
        "  internal_auc = roc_auc_score(y_test, y_proba[:, 1])\n",
        "\n",
        "  # Confusion Matrix\n",
        "  cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "  # Classification Report\n",
        "  report = classification_report(y_test, y_pred)\n",
        "\n",
        "  print(f\"Test Accuracy: {internal_accuracy:.4f}\")\n",
        "  print(f\"Test F1 Score: {internal_f1:.4f}\")\n",
        "  print(f\"Test ROC-AUC: {internal_auc:.4f}\\n\")\n",
        "\n",
        "  print(\"\\nClassification report:\\n\", report)\n",
        "\n",
        "  print(\"Confusion Matrix:\\n\", cm)\n",
        "\n",
        "  # return internal_accuracy, internal_f1, internal_auc\n"
      ],
      "metadata": {
        "id": "LZb46gN9rhcB"
      },
      "id": "LZb46gN9rhcB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_acc_f1_roc(y_test, y_pred_lr, y_proba_lr)\n"
      ],
      "metadata": {
        "id": "C64_GKQwcgsc"
      },
      "id": "C64_GKQwcgsc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EXTERNAL TEST"
      ],
      "metadata": {
        "id": "BwJXle87jXWL"
      },
      "id": "BwJXle87jXWL"
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = df_test[num_features + cat_features + bin_features]\n",
        "y_test = df_test['y'].astype(int)"
      ],
      "metadata": {
        "id": "kc9Xq2U3l9y3"
      },
      "id": "kc9Xq2U3l9y3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions\n",
        "y_pred_lr = best_lr.predict(X_test)\n",
        "y_proba_lr = best_lr.predict_proba(X_test)\n"
      ],
      "metadata": {
        "id": "JEg-aaLQjVra"
      },
      "id": "JEg-aaLQjVra",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_acc_f1_roc(y_test, y_pred_lr, y_proba_lr)"
      ],
      "metadata": {
        "id": "5aMxy3sltMgG"
      },
      "id": "5aMxy3sltMgG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plotting ROC_Curve"
      ],
      "metadata": {
        "id": "ltZkM0MQwTF3"
      },
      "id": "ltZkM0MQwTF3"
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary of models\n",
        "models = {\n",
        "    \"Logistic Regression\": best_lr\n",
        "}"
      ],
      "metadata": {
        "id": "IhAFaEldwbs5"
      },
      "id": "IhAFaEldwbs5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, model in models.items():\n",
        "    y_true_all = []\n",
        "    y_prob_all = []\n",
        "\n",
        "    for train_idx, test_idx in cv_eval.split(X, y):\n",
        "        est = clone(model)\n",
        "        est.fit(X.iloc[train_idx], y.iloc[train_idx])\n",
        "\n",
        "        # Predict probabilities for class 1\n",
        "        if hasattr(est, \"predict_proba\"):\n",
        "            y_prob = est.predict_proba(X.iloc[test_idx])[:, 1]\n",
        "        elif hasattr(est, \"decision_function\"):\n",
        "            y_prob = est.decision_function(X.iloc[test_idx])\n",
        "        else:\n",
        "            y_prob = est.predict(X.iloc[test_idx])  # fallback\n",
        "\n",
        "        y_true_all.append(y.iloc[test_idx])\n",
        "        y_prob_all.append(y_prob)\n",
        "\n",
        "    # Concatenate results from all folds\n",
        "    y_true_concat = np.concatenate(y_true_all)\n",
        "    y_prob_concat = np.concatenate(y_prob_all)\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(y_true_concat, y_prob_concat)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.plot(fpr, tpr, lw=2, label=f\"{name} (AUC = {roc_auc:.3f})\")\n",
        "\n",
        "\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curves for All Models')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DD87fjAvwSap"
      },
      "id": "DD87fjAvwSap",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}